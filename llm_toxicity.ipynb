{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbansaha96/AI-ML-Playground/blob/master/llm_toxicity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "P0p-217OUH1X"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets transformers evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XzqUfi-QOjC7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "dataset = load_dataset(\"Arsive/toxicity_classification_jigsaw\"\n",
        ", split=\"train\"\n",
        ")\n",
        "# textdetox/multilingual_toxicity_dataset\n",
        "# google/jigsaw_toxicity_pred\n",
        "# Arsive/toxicity_classification_jigsaw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "RsPxIHr6UIrg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "def evaluate_model(model_name, toxic_label, threshold=0.5):\n",
        "    toxicity = load(\"toxicity\", model_name, module_type=\"measurement\")\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "    binary_predictions = []\n",
        "    binary_ground_truth = []\n",
        "\n",
        "    for item in dataset:\n",
        "        text = item[\"comment_text\"] # item[\"prompt\"][\"text\"]\n",
        "        label = item[\"toxic\"] # item[\"prompt\"][\"toxicity\"]\n",
        "\n",
        "        # Compute toxicity score\n",
        "        prediction = toxicity.compute(predictions=[text], toxic_label=toxic_label)[\"toxicity\"][0]\n",
        "\n",
        "        predictions.append(prediction)\n",
        "        ground_truth.append(label)\n",
        "\n",
        "        # Convert to binary based on threshold\n",
        "        prediction_binary = 1 if prediction >= threshold else 0\n",
        "        label_binary = 1 if label >= threshold else 0\n",
        "\n",
        "        binary_predictions.append(prediction_binary)\n",
        "        binary_ground_truth.append(label_binary)\n",
        "\n",
        "    # Calculate classification metrics\n",
        "    f1 = f1_score(binary_ground_truth, binary_predictions)\n",
        "    precision = precision_score(binary_ground_truth, binary_predictions)\n",
        "    recall = recall_score(binary_ground_truth, binary_predictions)\n",
        "    accuracy = accuracy_score(binary_ground_truth, binary_predictions)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    # rmse = mean_squared_error(ground_truth, predictions)\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model_name,\n",
        "        # \"rmse\": rmse,\n",
        "        \"f1_score\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"accuracy\": accuracy\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time"
      ],
      "metadata": {
        "id": "_VbpPUfStvOA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw4Hn5BPbVpj"
      },
      "outputs": [],
      "source": [
        "model_results = {}\n",
        "total_start = time()\n",
        "models = [\n",
        "    {\"model_name\": \"facebook/roberta-hate-speech-dynabench-r4-target\", \"toxic_label\": \"hate\"},\n",
        "    {\"model_name\": \"DaNLP/da-electra-hatespeech-detection\", \"toxic_label\": \"offensive\"},\n",
        "    {\"model_name\": \"unitary/multilingual-toxic-xlm-roberta\", \"toxic_label\": \"toxic\"},\n",
        "    {\"model_name\": \"unitary/toxic-bert\", \"toxic_label\": \"toxic\"},\n",
        "    {\"model_name\": \"martin-ha/toxic-comment-model\", \"toxic_label\": \"toxic\"},\n",
        "    {\"model_name\": \"textdetox/xlmr-large-toxicity-classifier\", \"toxic_label\": \"LABEL_1\"},\n",
        "    {\"model_name\": \"s-nlp/roberta_toxicity_classifier\", \"toxic_label\": \"toxic\"}\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    try:\n",
        "        model_start_time = time()\n",
        "        metric_scores = evaluate_model(model_name=model[\"model_name\"], toxic_label=model[\"toxic_label\"])\n",
        "        model_results[model[\"model_name\"]] = metric_scores\n",
        "        model_end_time = time()\n",
        "        print(f\"Model Time Taken - {model_end_time-model_start_time}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Toxicity Calculation failed for {model['model_name']} with toxicity_label {model['toxic_label']} with error {e}\")\n",
        "\n",
        "total_end = time()\n",
        "\n",
        "print(f\"Total Time Taken - {total_end-total_start}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOzRAZkvgEWt"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "print(json.dumps(model_results, indent=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Extract model names and metric values\n",
        "model_names = [model_results[model][\"model_name\"] for model in model_results]\n",
        "f1_scores = [model_results[model][\"f1_score\"] for model in model_results]\n",
        "precisions = [model_results[model][\"precision\"] for model in model_results]\n",
        "recalls = [model_results[model][\"recall\"] for model in model_results]\n",
        "accuracies = [model_results[model][\"accuracy\"] for model in model_results]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# F1 Score Plot\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.barh(model_names, f1_scores, color='skyblue')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the highest score at the top\n",
        "plt.title('F1 Score')\n",
        "\n",
        "# Precision Plot\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.barh(model_names, precisions, color='salmon')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis\n",
        "plt.title('Precision')\n",
        "\n",
        "# Recall Plot\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.barh(model_names, recalls, color='lightgreen')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis\n",
        "plt.title('Recall')\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.barh(model_names, accuracies, color='gold')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NmqRjaobtCyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0_ia4ZIbX-x"
      },
      "outputs": [],
      "source": [
        "# sorted_models = sorted(model_results.items(), key=lambda x: x[1])\n",
        "\n",
        "# print(\"Model Performance (Best to Worst):\")\n",
        "# for model, score in sorted_models:\n",
        "#   print(f\"{model}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X0nmmKS3uDH"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "# toxic_classifier = pipeline(\"text-classification\", model=\"textdetox/xlmr-large-toxicity-classifier\", top_k=99999, truncation=True)\n",
        "# toxic_classifier.model.config.id2label"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMXXl1/PiS/g2zeskII46AK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}